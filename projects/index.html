<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="preload" href="/sajadabvi.github.io/_next/static/css/ac57406238b90d1e.css" as="style"/><link rel="stylesheet" href="/sajadabvi.github.io/_next/static/css/ac57406238b90d1e.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/sajadabvi.github.io/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/sajadabvi.github.io/_next/static/chunks/webpack-0e4399a7489fe047.js" defer=""></script><script src="/sajadabvi.github.io/_next/static/chunks/framework-d7f578ab3069408c.js" defer=""></script><script src="/sajadabvi.github.io/_next/static/chunks/main-8a83d899abdff178.js" defer=""></script><script src="/sajadabvi.github.io/_next/static/chunks/pages/_app-8a28ec9e8ed59ce2.js" defer=""></script><script src="/sajadabvi.github.io/_next/static/chunks/pages/projects-013258352482e836.js" defer=""></script><script src="/sajadabvi.github.io/_next/static/e3fW9f998mwbTOB53KD4y/_buildManifest.js" defer=""></script><script src="/sajadabvi.github.io/_next/static/e3fW9f998mwbTOB53KD4y/_ssgManifest.js" defer=""></script></head><body><link rel="preload" as="image" href="/images/gracec1.jpeg"/><link rel="preload" as="image" href="/images/gracec2.jpeg"/><link rel="preload" as="image" href="/images/gracec3.jpeg"/><link rel="preload" as="image" href="/images/clearr_figure2.jpeg"/><link rel="preload" as="image" href="/images/clearr_figure3.jpeg"/><link rel="preload" as="image" href="/images/clearr_figure4.jpeg"/><link rel="preload" as="image" href="/images/autoaugment_fig2.jpeg"/><link rel="preload" as="image" href="/images/decaps_fig1.jpeg"/><link rel="preload" as="image" href="/images/causal_puzzle_fig1.jpeg"/><link rel="preload" as="image" href="/images/causal_puzzle_fig5.jpeg"/><link rel="preload" as="image" href="/images/ionc_fig4.jpeg"/><div id="__next"><div class="min-h-screen flex flex-col"><header class="bg-gray-800 text-white"><nav class="container mx-auto px-4 py-4 flex flex-wrap justify-between items-center"><div class="font-bold text-xl">Mohammadsajad Abavisani</div><div class="space-x-4"><a href="/sajadabvi.github.io">Home</a><a href="/sajadabvi.github.io/research/">Research</a><a href="/sajadabvi.github.io/projects/">Projects</a><a href="/sajadabvi.github.io/resume/">Resume</a><a href="/sajadabvi.github.io/skills/">Skills</a><a href="/sajadabvi.github.io/blog/">Blog</a><a href="/sajadabvi.github.io/contact/">Contact</a><a href="/sajadabvi.github.io/media/">Media</a><a href="/sajadabvi.github.io/photography/">Photography</a></div></nav></header><main class="flex-grow container mx-auto px-4 py-8"><div><h1 class="text-3xl font-bold mb-4">Projects &amp; Portfolio</h1><p class="mb-4">Click on a project to view more details.</p><div class="mb-6"><h2 class="text-2xl font-bold mb-2">Research Projects</h2><div class="space-y-4"><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">GRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>Discover GRACE-C, a breakthrough method that redefines causal discovery in undersampled time series data. By bridging the gap between the true causal timescale and slower measurement rates, GRACE-C uncovers hidden dynamics in complex systems—be it brain networks or economic models.</p><p>Harnessing advanced constraint programming and cutting‐edge theoretical insights, GRACE-C delivers up to three orders of magnitude speed improvement over previous methods. It scales effortlessly to graphs with over 100 nodes and robustly handles noisy, real-world data.</p><p><strong>Key Results:</strong></p><ul class="list-disc ml-6"><li>Delivers a dramatic speed boost compared to existing techniques.</li><li>Efficiently processes graphs with 100+ nodes without needing undersampling rates.</li><li>Maintains high accuracy despite statistical noise and potential edge misidentification.</li></ul><p class="mt-2">Experience a new era in causal discovery—where innovation meets real-world application.</p><div class="mt-4 grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-4"><div class="flex flex-col items-center"><img src="/images/gracec1.jpeg" alt="GRACE-C Graph Overview" class="w-full h-auto rounded"/><p class="text-sm text-center mt-2">Visualizing GRACE-C’s intricate causal graph.</p></div><div class="flex flex-col items-center"><img src="/images/gracec2.jpeg" alt="Speed Comparison of sRASL vs. RASL" class="w-full h-auto rounded"/><p class="text-sm text-center mt-2">Speed boost: GRACE-C outperforms traditional methods.</p></div><div class="flex flex-col items-center"><img src="/images/gracec3.jpeg" alt="Performance Benchmark: SCC Impact on Computation Time" class="w-full h-auto rounded"/><p class="text-sm text-center mt-2">Figure 5 Benchmark: SCC structure’s impact on computation time.</p></div></div><div class="mt-6 flex flex-col items-center"><a href="https://openreview.net/forum?id=B_pCIsX8KL_" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/sajadabvi/gunfolds" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Causal Learning through Deliberate Undersampling<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>What if measuring data <strong>less frequently</strong> actually gave you <strong>more information</strong> about causality? This paper presents a groundbreaking idea that challenges the common belief that <strong>higher measurement rates are always better</strong>. Instead, by strategically <strong>undersampling</strong>, we can reduce ambiguity and gain a sharper understanding of causal relationships!</p><p><strong>Key Results:</strong></p><ul class="list-disc ml-6"><li><strong>Hidden Insights Unlocked:</strong> Slower measurements can provide <strong>a different perspective on causality</strong>, revealing dependencies that are otherwise obscured.</li><li><strong>Reduced Equivalence Classes:</strong> By including undersampled data, we significantly <strong>reduce the number of possible causal graphs</strong>, making structure learning more precise.</li><li><strong>Non-Monotonic Effects:</strong> Counterintuitively, adding a <strong>slower dataset</strong> can sometimes <strong>increase accuracy more than adding a faster dataset!</strong></li></ul><p class="mt-2">Below, the images illustrate how undersampling affects equivalence classes and quantifies the measurable gains achieved through this method.</p><div class="mt-4 grid grid-cols-1 sm:grid-cols-2 gap-4"><div><img src="/images/clearr_figure2.jpeg" alt="Equivalence Class Reduction via Undersampling" class="w-full h-auto rounded"/><p class="text-center mt-2 italic">Venn diagrams showing how undersampling affects equivalence class relationships, revealing hidden causal structures.</p></div></div><div class="mt-4 grid grid-cols-1 sm:grid-cols-2 gap-4"><div><img src="/images/clearr_figure3.jpeg" alt="Histogram Showing Graph Properties" class="w-full h-auto rounded"/><p class="text-center mt-2 italic">Density estimation showing how graph structures change with different undersampling rates.</p></div><div><img src="/images/clearr_figure4.jpeg" alt="Histogram Quantifying Undersampling Gain" class="w-full h-auto rounded"/><p class="text-center mt-2 italic">Histogram quantifying the gain from undersampling, measuring improvement in causal structure identification.</p></div></div><div class="mt-6 flex flex-col items-center"><a href="https://proceedings.mlr.press/v213/solovyeva23a.html" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/sajadabvi/gunfolds" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Greedy AutoAugment<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>Greedy AutoAugment redefines image data augmentation by employing a highly efficient greedy search strategy to discover optimal augmentation policies. Instead of exhaustively exploring an exponentially expanding search space, the algorithm leverages a greedy approach that reduces the number of trials to a manageable linear growth. This dynamic method efficiently selects and combines augmentation operations to boost the performance of neural networks on image classification tasks.</p><p>By integrating a breadth-first search for policy exploration with targeted policy evaluation, Greedy AutoAugment dramatically cuts down on computational requirements—achieving comparable or higher accuracies on challenging datasets such as Tiny ImageNet, CIFAR-10, CIFAR-100, and SVHN while using up to 360 times fewer computational resources.</p><div class="mt-4 flex flex-col items-center"><img src="/images/autoaugment_fig2.jpeg" alt="Fig. 2. The general scheme of AutoAugment algorithm." class="w-full h-auto rounded"/><p class="text-center mt-2 italic">Fig. 2. The general scheme of AutoAugment algorithm.</p></div><div class="mt-6 flex flex-col items-center"><a href="https://www.sciencedirect.com/science/article/pii/S0167865520303305" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/sajadabvi/greedy_auto_augment" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Radiologist-Level COVID-19 Detection Using CT Scans with Detail-Oriented Capsule Networks<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>Experience a breakthrough in automated COVID-19 diagnosis with the DECAPS architecture—a novel, detail-oriented capsule network designed to emulate the nuanced diagnostic process of expert radiologists. DECAPS combines the robust capabilities of capsule networks with innovative techniques such as Inverted Dynamic Routing and the Peekaboo training strategy to focus on critical regions within CT scans.</p><p>By selectively amplifying signals from regions of interest and incorporating both coarse and fine-grained feature representations, DECAPS achieves superior accuracy and sensitivity even with limited training data. The integration of conditional GAN-based data augmentation further bolsters its performance, enabling the system to outperform traditional methods and even experienced radiologists in key diagnostic metrics.</p><div class="mt-4 flex flex-col items-center"><img src="/images/decaps_fig1.jpeg" alt="Figure 1: (a): Illustration of the DECAPS architecture. Sample head activation maps (HAMs) are presented for the COVID-19 class. (b): Peekaboo training. HAMs are randomly selected to perform patch crop/drop while training. At test time, the average HAM across all heads is used to perform patch crop (with no patch drop required at test time) to obtain the fine-grained prediction. (c): The distillation process to fine-tune the coarse-grained prediction (pcoarse) using the fine-grained prediction (pfine) to obtain the final distilled predictions (pdist.)." class="w-full h-auto rounded"/><p class="text-center mt-2 italic">Figure 1: (a): Illustration of the DECAPS architecture with sample head activation maps for COVID-19. (b): Peekaboo training with patch crop/drop. (c): Distillation process for refining predictions.</p></div><div class="mt-6 flex flex-col items-center"><a href="https://arxiv.org/abs/2004.07407" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/amobiny/DECAPS_for_COVID19" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Piecing Together the Causal Puzzle<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>&quot;Piecing Together the Causal Puzzle&quot; confronts one of neuroimaging’s toughest challenges—recovering the true causal structure from undersampled, noisy time series data. When fMRI sampling intervals lag behind the millisecond-level neural interactions, traditional methods yield multiple plausible causal graphs. This work innovates by leveraging Answer Set Programming (ASP) to rigorously explore and prune the space of potential causal graphs.</p><p>By incorporating domain-specific constraints and an advanced optimization framework, the method doesn’t settle on a single “best” graph; instead, it produces an entire equivalence class of candidate solutions. This rich solution set enables experts to manually inspect and select the most plausible causal model, effectively piecing together the underlying causal puzzle from incomplete data.</p><p>Furthermore, the approach demonstrates robust performance on simulated fMRI data, accurately preserving the BOLD signal across varying undersampling rates. This ensures that even when data are sampled sparsely, critical neural interactions remain intact—a breakthrough for causal inference in neuroimaging.</p><div class="mt-4"><div class="flex flex-col items-center"><img src="/images/causal_puzzle_fig1.jpeg" alt="Figure 1: Top left(A): Size of optimization solution set across different undersamplings, repeated 100 times. Bottom right(B): Commission error of the solution vs. optimization cost for that solution. Solutions in one equivalent class are highlighted in red." class="w-full h-auto rounded"/><p class="text-center mt-2 italic">Figure 1: Top left (A) shows the size of the optimization solution set across different undersamplings (repeated 100 times), while bottom right (B) depicts the commission error versus optimization cost, with solutions in one equivalent class highlighted in red.</p></div><div class="flex flex-col items-center mt-6"><img src="/images/causal_puzzle_fig5.jpeg" alt="Figure 5: Impact of different undersampling rates (1s, 2s, 3s) on BOLD signal preservation in fMRI data simulated with the balloon model. Minimal error is observed with 1-second undersampling, whereas larger intervals degrade accuracy in all methods that don’t account for undersampling effect. Our method RnR accounts for this effect and does not suffer loss from undersampling." class="w-full h-auto rounded"/><p class="text-center mt-2 italic">Figure 5: Impact of different undersampling rates (1s, 2s, 3s) on BOLD signal preservation in fMRI data simulated with the balloon model. Minimal error is observed with 1-second undersampling, whereas larger intervals degrade accuracy in methods that ignore undersampling effects. Our method, RnR, overcomes this limitation.</p></div></div><div class="mt-6 flex flex-col items-center"><a href="/papers/Highway_to_Causality__Rock_n__RASL_Tunes_Up_Noisy_Data_with_Answer_Set_Programming.pdf" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/sajadabvi/gunfolds" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">ION-C: Integration of Overlapping Networks via Constraints<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>ION-C offers a powerful and efficient solution to one of causal inference’s most challenging puzzles—integrating information from multiple datasets that share only a subset of variables. By formulating the problem using Answer Set Programming (ASP), ION-C skillfully merges local causal graphs derived from overlapping datasets into a complete, globally consistent causal model.</p><p>This innovative approach not only enumerates all possible ground-truth directed acyclic graphs (DAGs) that respect the independence relations present in the input graphs but also significantly reduces computational complexity compared to earlier methods. The resulting solution set provides experts with a rich palette of candidate models, enabling more informed decisions about underlying causal structures.</p><p>ION-C’s versatility makes it ideal for applications ranging from social survey analysis to complex biomedical data, where variables are often measured across disjoint datasets. The method’s ability to pinpoint commonalities and reconcile differences among overlapping networks sets a new standard in causal graph recovery.</p><div class="mt-4 flex flex-col items-center"><img src="/images/ionc_fig4.jpeg" alt="Figure 4: Representation of ION solution set." class="w-full h-auto rounded"/><p class="text-center mt-2 italic">Figure 4: Representation of ION solution set.</p></div><div class="mt-6 flex flex-col items-center"><a href="https://arxiv.org/abs/2411.04243" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/neuroneural/gunfolds" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div></div></div><div class="mb-6"><h2 class="text-2xl font-bold mb-2">Technical Projects</h2><div class="space-y-4"><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Large Language Models &amp; Knowledge Graphs<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>Worked on integrating Llama 3 with knowledge graphs to improve logical reasoning and problem-solving in AI models.</p><div class="mt-6 flex flex-col items-center"><a href="https://example.com/fullpaper-llama-kg" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/username/llama-kg" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Generative Adversarial Networks (GANs) for Self-Driving Cars<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>Trained GANs to augment and improve the classification of street signs under diverse weather conditions using multimodal inputs (image &amp; text).</p><div class="mt-6 flex flex-col items-center"><a href="https://example.com/fullpaper-gan-selfdriving" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/username/gan-selfdriving" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Topology-Aware Self-Supervised Learning<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>Developed a contrastive learning framework with a topology loss term to reduce texture bias in CNNs, improving generalization in vision tasks.</p><div class="mt-6 flex flex-col items-center"><a href="https://example.com/fullpaper-topology-ssl" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/username/topology-ssl" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Dynamical System Prediction with Neural Networks<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>Built neural networks (PhICNet) to predict physical systems governed by PDEs, integrating multimodal physics and image data for enhanced spatio-temporal forecasting.</p><div class="mt-6 flex flex-col items-center"><a href="https://example.com/fullpaper-dynamic-prediction" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/username/dynamic-prediction" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div><div class="border p-4 rounded-lg shadow-md bg-white"><button class="w-full text-left font-semibold text-lg flex justify-between items-center">Eye Gaze Tracking Device<span>▼</span></button><div class="transition-all duration-300 overflow-auto max-h-0 opacity-0"><div class="mt-2"><div><p>Designed an eye-tracking device from scratch using C#.NET, OpenCV, Emgu, and AForge libraries. Achieved high-accuracy gaze detection for real-time applications.</p><div class="mt-6 flex flex-col items-center"><a href="https://example.com/fullpaper-eye-gaze" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold">Read Full Paper</a><a href="https://github.com/username/eye-gaze-tracking" target="_blank" rel="noopener noreferrer" class="text-blue-500 underline text-lg font-semibold mt-2">Code</a></div></div></div></div></div></div></div></div></main><footer class="bg-gray-800 text-white py-4 text-center">© <!-- -->2025<!-- --> Mohammadsajad Abavisani. All rights reserved.</footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/projects","query":{},"buildId":"e3fW9f998mwbTOB53KD4y","assetPrefix":"/sajadabvi.github.io","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>